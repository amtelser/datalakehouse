# üìä Datalakehouse IoT Telematics

Pipeline anal√≠tico para telemetr√≠a GPS usando un enfoque **Lakehouse** (Iceberg + Nessie) con procesamiento **streaming** y **batch** sobre Flink y consultas interactivas en Trino.

## üß± Arquitectura (alto nivel)
| Flujo | Componente | Rol |
|-------|------------|-----|
| Ingesta | Confluent Kafka | Stream de eventos decodificados |
| Procesamiento streaming | Flink SQL | Inserci√≥n en `gps_reports` y mantenimiento `latest_gps_by_device` |
| Procesamiento streaming | Flink SQL | Inserci√≥n en `raw and dlq` |
| Batch diario | Flink SQL | C√°lculo de score de riesgo (`risk_score_daily`) |
| Cat√°logo | Nessie | Versionado (branches, commits, snapshots) |
| Formato / Tablas | Apache Iceberg | Tablas ACID particionadas / evoluci√≥n de esquema |
| Almacenamiento | S3 | Data Lake (archivos Parquet) |
| Metastore Nessie | Postgres | Persistencia de metadatos |
| SQL interactivo | Trino | Exploraci√≥n / BI |
| API | FastAPI (telematics_api) | Exposici√≥n REST de datos (Trino) |

---

## üöÄ Quick Start
1. Requisitos: Docker + Docker Compose.
2. Levantar servicios base:
  ```bash
  docker compose up -d
  ```
3. Confirmar UI:
  - Flink: http://localhost:8081/
  - Trino: http://localhost:8080/
  - Nessie (UI): http://localhost:19120/content/main/telematics/gps_reports
4. Ejecutar SQL de creaci√≥n (cat√°logo + tablas): ver secci√≥n siguiente.
5. Lanzar jobs streaming.
6. (Opcional) Ejecutar job batch de riesgo.
7. Consultar en Trino o v√≠a API.

---

## üìÇ Archivos SQL (carpeta `config/flink/`)
- `create.sql`: crea cat√°logo Nessie, DB `telematics`, tablas Iceberg y fuentes temporales (Kafka / JDBC Postgres).
- `gps_reports.sql`: job streaming ‚Üí ingesta Kafka ‚Üí Iceberg (`gps_reports`).
- `latest_gps_by_device.sql`: job streaming ‚Üí tabla upsert con √∫ltima posici√≥n por dispositivo.
- `telematics_raw_dlq.sql`: job streaming ‚Üí ingesta Kafka ‚Üí Iceberg (`raw and dlq`).
- `risk_score_daily.sql`: job batch ‚Üí calcula score y escribe en tabla Iceberg `risk_score_daily`.
- `cleanup_raw_dlq.sql` (si se usa) / scripts auxiliares.

---

## ‚ñ∂Ô∏è Ejecuci√≥n de Jobs (desde contenedor Flink `jobmanager`)

### 1. Crear cat√°logo + tablas
```bash
docker exec -it jobmanager bash -lc "bin/sql-client.sh -f /opt/sql/create.sql"
```

### 2. Streaming ‚Üí Ingesta `gps_reports`
```bash
docker exec -it jobmanager bash -lc "bin/sql-client.sh -i /opt/sql/create.sql -f /opt/sql/gps_reports.sql"
```

### 3. Streaming ‚Üí Ingesta `raw and dlq`
```bash
docker exec -it jobmanager bash -lc "bin/sql-client.sh -i /opt/sql/create.sql -f /opt/sql/telematics_raw_dlq.sql"
```

### 4. Streaming ‚Üí Tabla `latest_gps_by_device`
```bash
docker exec -it jobmanager bash -lc "bin/sql-client.sh -i /opt/sql/create.sql -f /opt/sql/latest_gps_by_device.sql"
```

### 5. Batch diario ‚Üí Score de riesgo (elige destino)
Iceberg (tabla `telematics.risk_score_daily`):
```bash
docker exec -it jobmanager bash -lc "bin/sql-client.sh -i /opt/sql/create.sql -f /opt/sql/risk_score_daily.sql"
```

#### Ajustar rango de fechas
Ambos scripts definen un CTE `rango`:
```sql
WITH rango AS (
  SELECT DATE 'YYYY-MM-DD' AS d_ini, DATE 'YYYY-MM-DD' AS d_fin
)
```
Modifica `d_ini` y `d_fin` antes de lanzar el job para recalcular un intervalo hist√≥rico. (Podr√≠as parametrizar en el futuro usando variables externas o plantillas.)

---

## üîç Consultas en Trino
Abrir CLI:
```bash
docker exec -it trino trino
```
Ejemplos:
```sql
SHOW TABLES IN nessie.telematics;

SELECT * FROM nessie.telematics.gps_reports ORDER BY received_epoch DESC LIMIT 10;

SELECT * FROM nessie.telematics.latest_gps_by_device LIMIT 20;

-- Ajusta la fecha (columna report_date)
SELECT *
FROM nessie.telematics.risk_score_daily
WHERE report_date = DATE '2025-08-31'
ORDER BY score DESC
LIMIT 20;
```

---

## üß™ API REST (`telematics_api`)
Levantar s√≥lo la API (si ya est√° el stack principal):
```bash
docker compose up -d telematics_api
```
Endpoints (requieren header `Authorization: Bearer token1` por defecto de ejemplo):
- `GET /health`
- `GET /gps_reports?limit=100`
- `GET /gps_reports?device_id=...&from_ts=...&to_ts=...`
- `GET /latest_gps_by_device?device_id=...`
- `GET /risk_score_daily?day=YYYY-MM-DD`

Ejemplos:
```bash
curl -H "Authorization: Bearer token1" http://localhost:9009/health
curl -H "Authorization: Bearer token1" "http://localhost:9009/gps_reports?limit=50"
```

---

## üõ†Ô∏è Mantenimiento / Utilidades
- `scripts/cleanup_gps_reports.sh`: ejemplo de limpieza / utilitario (ajustar antes de usar).
- Particiones Iceberg: revisar en S3 o v√≠a `DESCRIBE TABLE` en Trino.
- Actualizaci√≥n de credenciales: externalizar en variables / `.env` (actualmente algunos valores est√°n embebidos en `create.sql`).

---

## ‚ö†Ô∏è Observaciones T√©cnicas / TODO
1. `create.sql` define partici√≥n `'partitioning' = 'score_date, bucket(1024, device_id)'` para `risk_score_daily`, pero la columna se llama `report_date`. Verificar y alinear (renombrar a `score_date` o ajustar partici√≥n).
2. Externalizar secretos Kafka / Postgres (no versionar credenciales reales).
3. Parametrizar rango de fechas del job batch (ej: pasar como variables de entorno y hacer template).
4. A√±adir tests ligeros para API (FastAPI + pytest) y un Makefile.
5. Monitoreo: integrar Flink metrics / Prometheus (opcional futuro).

---

## üìå Notas importantes
- Jobs streaming se mantienen activos; al reiniciar el cluster se deben volver a lanzar si no hay savepoints configurados.
- El job batch puede re-ejecutarse m√∫ltiples veces para recalcular (upsert) el intervalo definido.
- Evitar `docker compose down -v` para no borrar vol√∫menes (Postgres + datos Iceberg).
- Usar ramas / commits Nessie para experimentos (ej: crear branch y comparar outputs de scoring).

---

## üîé Troubleshooting r√°pido
| Problema | Causa t√≠pica | Acci√≥n |
|----------|--------------|--------|
| Tabla vac√≠a en Trino | Job streaming no iniciado | Revisar Jobs en UI Flink / relanzar |
| Error conector Kafka | Credenciales / offsets | Validar config en `create.sql` y conectividad | 
| No aparece commit Nessie | Job no escribi√≥ / fall√≥ | Logs del job en Flink / revisar excepciones |
| Duplicados en latest | Upsert no activo | Confirmar `write.upsert.enabled` y PK NOT ENFORCED |

---

## üìí Referencia r√°pida (cheat sheet)
```bash
# Crear objetos base
docker exec -it jobmanager bash -lc "bin/sql-client.sh -f /opt/sql/create.sql"

# Jobs streaming
docker exec -it jobmanager bash -lc "bin/sql-client.sh -i /opt/sql/create.sql -f /opt/sql/gps_reports.sql"
docker exec -it jobmanager bash -lc "bin/sql-client.sh -i /opt/sql/create.sql -f /opt/sql/latest_gps_by_device.sql"

# Batch riesgo (Iceberg)
docker exec -it jobmanager bash -lc "bin/sql-client.sh -i /opt/sql/create.sql -f /opt/sql/risk_score_daily.sql"

# Trino CLI
docker exec -it trino trino
```

---

## ‚úÖ Estado actual
- Ingesta y tablas Iceberg: OK
- Latest por dispositivo: OK (upsert habilitado)
- Score riesgo: scripts duales (Iceberg/Postgres) operativos
- API: disponible para consultas b√°sicas

---

Contribuciones / mejoras bienvenidas. Mantener consistencia de estilo SQL y evitar credenciales hardcode en futuros cambios.