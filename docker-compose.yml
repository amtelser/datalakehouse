# docker-compose.yml

x-mounts:
  lib_iceberg_flink_runtime: &lib_iceberg_flink_runtime
    type: bind
    source: ./runtime/lib/iceberg-flink-runtime-1.20-1.9.2.jar
    target: /opt/flink/lib/iceberg-flink-runtime-1.20-1.9.2.jar
    read_only: true
  lib_iceberg_aws_bundle: &lib_iceberg_aws_bundle
    type: bind
    source: ./runtime/lib/iceberg-aws-bundle-1.9.2.jar
    target: /opt/flink/lib/iceberg-aws-bundle-1.9.2.jar
    read_only: true
  lib_iceberg_nessie: &lib_iceberg_nessie
    type: bind
    source: ./runtime/lib/iceberg-nessie-1.9.2.jar
    target: /opt/flink/lib/iceberg-nessie-1.9.2.jar
    read_only: true
  lib_flink_kafka: &lib_flink_kafka
    type: bind
    source: ./runtime/lib/flink-connector-kafka-4.0.1-2.0.jar
    target: /opt/flink/lib/flink-connector-kafka-4.0.1-2.0.jar
    read_only: true
  lib_kafka_clients: &lib_kafka_clients
    type: bind
    source: ./runtime/lib/kafka-clients-3.9.1.jar
    target: /opt/flink/lib/kafka-clients-3.9.1.jar
    read_only: true
  lib_flink_s3_hadoop: &lib_flink_s3_hadoop
    type: bind
    source: ./runtime/lib/flink-s3-fs-hadoop-1.20.2.jar
    target: /opt/flink/lib/flink-s3-fs-hadoop-1.20.2.jar
    read_only: true
  lib_flink_hadoop_fs: &lib_flink_hadoop_fs
    type: bind
    source: ./runtime/lib/flink-hadoop-fs-1.20.2.jar
    target: /opt/flink/lib/flink-hadoop-fs-1.20.2.jar
    read_only: true
  lib_hadoop_common: &lib_hadoop_common
    type: bind
    source: ./runtime/lib/hadoop-common-3.3.6.jar
    target: /opt/flink/lib/hadoop-common-3.3.6.jar
    read_only: true
  lib_hadoop_hdfs_client: &lib_hadoop_hdfs_client
    type: bind
    source: ./runtime/lib/hadoop-hdfs-client-3.3.6.jar
    target: /opt/flink/lib/hadoop-hdfs-client-3.3.6.jar
    read_only: true
  lib_hadoop_auth: &lib_hadoop_auth
    type: bind
    source: ./runtime/lib/hadoop-auth-3.3.6.jar
    target: /opt/flink/lib/hadoop-auth-3.3.6.jar
    read_only: true
  lib_hadoop_mr_core: &lib_hadoop_mr_core
    type: bind
    source: ./runtime/lib/hadoop-mapreduce-client-core-3.3.6.jar
    target: /opt/flink/lib/hadoop-mapreduce-client-core-3.3.6.jar
    read_only: true
  lib_hadoop_mr_common: &lib_hadoop_mr_common
    type: bind
    source: ./runtime/lib/hadoop-mapreduce-client-common-3.3.6.jar
    target: /opt/flink/lib/hadoop-mapreduce-client-common-3.3.6.jar
    read_only: true
  lib_hadoop_mr_jobclient: &lib_hadoop_mr_jobclient
    type: bind
    source: ./runtime/lib/hadoop-mapreduce-client-jobclient-3.3.6.jar
    target: /opt/flink/lib/hadoop-mapreduce-client-jobclient-3.3.6.jar
    read_only: true
  lib_postgres: &lib_postgres
    type: bind
    source: ./runtime/lib/postgresql-42.7.3.jar
    target: /opt/flink/lib/postgresql-42.7.3.jar
    read_only: true
  lib_flink_sql_jdbc: &lib_flink_sql_jdbc
    type: bind
    source: ./runtime/lib/flink-connector-jdbc-3.3.0-1.20.jar
    target: /opt/flink/lib/flink-connector-jdbc-3.3.0-1.20.jar
    read_only: true
  trino_conf: &trino_conf
    type: bind
    source: ./config/trino
    target: /etc/trino
    read_only: true

services:
  postgres:
    image: postgres:14
    environment:
      POSTGRES_DB: nessie
      POSTGRES_USER: nessie
      POSTGRES_PASSWORD: nessie
    volumes:
      - pg-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U nessie -d nessie"]
      interval: 5s
      timeout: 3s
      retries: 20

  nessie:
    image: ghcr.io/projectnessie/nessie:0.104.2-java
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "19120:19120"
    environment:
      QUARKUS_HTTP_PORT: "19120"
      NESSIE_VERSION_STORE_TYPE: JDBC
      QUARKUS_DATASOURCE_JDBC_URL: jdbc:postgresql://postgres:5432/nessie
      QUARKUS_DATASOURCE_USERNAME: nessie
      QUARKUS_DATASOURCE_PASSWORD: nessie
      NESSIE_CATALOG_DEFAULT_WAREHOUSE: "warehouse"
      NESSIE_CATALOG_WAREHOUSES_WAREHOUSE_LOCATION: "s3://iothub-telematics-data-stg/warehouse"
      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_ENDPOINT: "https://s3.us-west-2.amazonaws.com"
      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_PATH_STYLE_ACCESS: "true"
      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_REGION: "us-west-2"
      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_ACCESS_KEY: ""
      NESSIE_CATALOG_SERVICE_S3_DEFAULT_OPTIONS_SECRET_KEY: ""

  trino:
    image: trinodb/trino:476
    depends_on: [nessie]
    ports:
      - "8080:8080"
    volumes:
      - *trino_conf
      - trino-data:/data/trino
      - ./config/flink:/opt/sql:ro
    secrets:
      - source: trino_keystore
        target: trino_keystore.jks
      - source: trino_passwords
        target: trino_passwords.db
      - source: trino_rules
        target: trino_rules.json

  jobmanager:
    image: flink:1.20.2-scala_2.12
    container_name: jobmanager
    command: jobmanager
    ports:
      - "8081:8081"
    environment:
      - |
        FLINK_PROPERTIES=
        # Core
        jobmanager.rpc.address: jobmanager
        parallelism.default: 8

        # Checkpointing / Exactly-once
        execution.checkpointing.interval: 30 s
        execution.checkpointing.mode: EXACTLY_ONCE
        execution.checkpointing.externalized-checkpoint-retention: RETAIN_ON_CANCELLATION
        execution.checkpointing.max-concurrent-checkpoints: 1
        execution.checkpointing.tolerable-failed-checkpoints: 3

        # Persistir checkpoints/savepoints en s3 (muy importante)
        state.checkpoints.dir: s3://iothub-telematics-data-stg/flink/checkpoints
        state.savepoints.dir:  s3://iothub-telematics-data-stg/flink/savepoints

        # Reinicios
        restart-strategy: fixed-delay
        restart-strategy.fixed-delay.attempts: 10
        restart-strategy.fixed-delay.delay: 5 s

        # S3
        s3.endpoint: https://s3.us-west-2.amazonaws.com
        s3.path-style-access: true
        s3.access-key: 
        s3.secret-key: 
        s3.ssl.enabled: true
        fs.default-scheme: s3://iothub-telematics-data-stg

        # Memoria
        jobmanager.memory.process.size: 12288m
        jobmanager.memory.jvm-metaspace.size: 4096m
        jobmanager.memory.jvm-overhead.fraction: 0.10
        
        # Heartbeat
        heartbeat.interval: 10000
        heartbeat.timeout: 180000
      - TZ=America/Mexico_City
      - AWS_REGION=us-west-2
      - AWS_DEFAULT_REGION=us-west-2
      - AWS_ACCESS_KEY_ID=
      - AWS_SECRET_ACCESS_KEY=
      - AWS_EC2_METADATA_DISABLED=true
      - AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT=true
    volumes:
      - *lib_iceberg_flink_runtime
      - *lib_iceberg_aws_bundle
      - *lib_iceberg_nessie
      - *lib_flink_kafka
      - *lib_kafka_clients
      - *lib_flink_s3_hadoop
      - *lib_flink_hadoop_fs
      - *lib_hadoop_common
      - *lib_hadoop_hdfs_client
      - *lib_hadoop_auth
      - *lib_hadoop_mr_core
      - *lib_hadoop_mr_common
      - *lib_hadoop_mr_jobclient
      - *lib_postgres
      - *lib_flink_sql_jdbc
      - ./config/flink:/opt/sql:ro
    depends_on: [nessie]

  taskmanager:
    image: flink:1.20.2-scala_2.12
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        # Core
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 8
        taskmanager.memory.process.size: 12288m
        taskmanager.memory.managed.size: 4096m
        taskmanager.memory.network.min: 1024m
        taskmanager.memory.network.max: 2048m
        # S3 (para FS Hadoop)
        s3.endpoint: https://s3.us-west-2.amazonaws.com
        s3.path-style-access: true
        s3.access-key: 
        s3.secret-key: 
        s3.ssl.enabled: true
        # Evita HDFS por defecto
        fs.default-scheme: s3://iothub-telematics-data-stg
        heartbeat.interval: 10000
        heartbeat.timeout: 180000
      - TZ=America/Mexico_City
      - AWS_REGION=us-west-2
      - AWS_DEFAULT_REGION=us-west-2
      - AWS_ACCESS_KEY_ID=
      - AWS_SECRET_ACCESS_KEY=
      - AWS_EC2_METADATA_DISABLED=true
      - AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT=true
    volumes:
      - *lib_iceberg_flink_runtime
      - *lib_iceberg_aws_bundle
      - *lib_iceberg_nessie
      - *lib_flink_kafka
      - *lib_kafka_clients
      - *lib_flink_s3_hadoop
      - *lib_flink_hadoop_fs
      - *lib_hadoop_common
      - *lib_hadoop_hdfs_client
      - *lib_hadoop_auth
      - *lib_hadoop_mr_core
      - *lib_hadoop_mr_common
      - *lib_hadoop_mr_jobclient
      - *lib_postgres
      - *lib_flink_sql_jdbc
    depends_on: [jobmanager]

  telematics_api:
    build: ./services/telematics_api
    container_name: telematics_api
    depends_on: [trino]
    environment:
      - TRINO_HOST=trino
      - TRINO_PORT=8080
      - TRINO_USER=analyst
      - TRINO_CATALOG=nessie
      - TRINO_SCHEMA=telematics
      - TIME_ZONE=America/Mexico_City
      - API_TOKENS=token1,token2,token3
      - API_WORKERS=2
      - ALLOW_ORIGINS=http://localhost:9009,http://localhost:8080
    ports:
      - "9009:9009"

  spark:
    image: apache/spark:3.5.7-scala2.12-java17-python3-r-ubuntu
    container_name: spark
    user: root
    command: ["bash","-lc","sleep infinity"] 
    environment:
      - SPARK_LOCAL_IP=spark
      - TZ=America/Mexico_City
      - AWS_REGION=us-west-2
      - AWS_DEFAULT_REGION=us-west-2
      - AWS_ACCESS_KEY_ID=
      - AWS_SECRET_ACCESS_KEY=
    volumes:
      - ./runtime/lib:/opt/jars:ro
      - ./config/spark:/opt/jobs:ro
    depends_on: [nessie]

volumes:
  pg-data:
  trino-data:

secrets:
  trino_keystore:
    file: ./config/trino/keystore.jks
  trino_passwords:
    file: ./config/trino/password.db
  trino_rules:
    file: ./config/trino/rules.json